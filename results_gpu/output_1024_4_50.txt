2025-08-01 13:04:36.508768: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:542] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted.
2025-08-01 13:04:36.510287: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1762] INTERNAL: All tasks were set to error because coordination service encountered an error, but was unable to inform clients. Error: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted. [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
E0801 13:04:36.512265   46454 coordination_service_agent.cc:567] Polled an error from coordination service (this can be an error from this or another task).
2025-08-01 13:04:36.512726: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:542] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. Existing error: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted. [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker']
2025-08-01 13:04:36.512761: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1752] Use error polling to propagate the following error to all tasks: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. Existing error: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted. [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker'] [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker']
2025-08-01 13:04:36.513247: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1522] The following 1 tasks in the cluster has not sent request to poll for error. Error will not be propagated to these tasks: /job:jax_worker/replica:0/task:0
2025-08-01 13:04:36.515123: F external/xla/xla/pjrt/distributed/client.h:80] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: FAILED_PRECONDITION: Task (/job:jax_worker/replica:0/task:0) that is already in error state polling for errors. Current error: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted. [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker']

RPC: /tensorflow.CoordinationService/PollForError [type.googleapis.com/tensorflow.CoordinationServiceError='']
2025-08-01 13:04:36.515311: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:542] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. Existing error: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted. [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker']
2025-08-01 13:04:36.515328: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1752] Use error polling to propagate the following error to all tasks: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. Existing error: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted. [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker'] [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker']
2025-08-01 13:04:36.517305: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:542] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. Existing error: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted. [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker']
2025-08-01 13:04:36.517315: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1752] Use error polling to propagate the following error to all tasks: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. Existing error: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted. [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker'] [type.googleapis.com/tensorflow.CoordinationServiceError='\"\x0c\n\njax_worker']

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 45904 RUNNING AT l50015
=   EXIT CODE: 9
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Terminated (signal 15)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions

********************************************************************************
*                                                                              *
*  This is the automated job summary provided by DKRZ.                         *
*  If you encounter problems, need assistance or have any suggestion, please   *
*  write an email to                                                           *
*                                                                              *
*  --  support@dkrz.de --                                                      *
*                                                                              *
*                       We hope you enjoyed the DKRZ supercomputer LEVANTE ... *
*
* JobID            : 18796919
* JobName          : 1024_4_50                                         
* Account          : bk1377_gpu
* User             : a270230 (275572), ab0995 (1221)                   
* Partition        : gpu
* QOS              : normal
* Nodelist         : l50015 (1)                                                
* Submit date      : 2025-08-01T13:04:18
* Start time       : 2025-08-01T13:04:29
* End time         : 2025-08-01T13:04:36
* Elapsed time     : 00:00:07 (Timelimit=04:00:00)                     
* Command          : /home/a/a270230/veris_minimum_working_example/run_files/
*                    job_1024_4_50.sh
* WorkDir          : /home/a/a270230/veris_minimum_working_example
*
* StepID | JobName      NodeHours    MaxRSS [Byte] (@task)
* ------------------------------------------------------------------------------
* 0      | hydra_pmi_pr    0.0019                7648K (0)
* batch  | batch           0.0019
* extern | extern          0.0019                2160K (0)
* ------------------------------------------------------------------------------

